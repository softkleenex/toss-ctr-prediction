# Toss NEXT ML Challenge - CTR Prediction

í† ìŠ¤ ê´‘ê³  í´ë¦­ë¥  ì˜ˆì¸¡ AI ê²½ì§„ëŒ€íšŒ ì°¸ê°€ í”„ë¡œì íŠ¸

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

- **ëŒ€íšŒ**: Toss NEXT 2025 ML Challenge - ê´‘ê³  í´ë¦­ë¥ (CTR) ì˜ˆì¸¡
- **ì£¼ìµœ**: Dacon x í† ìŠ¤
- **ê¸°ê°„**: 2025.09 ~ 2025.10
- **ëª©í‘œ**: í† ìŠ¤ ì•± ë‚´ ê´‘ê³  í´ë¦­ í™•ë¥  ì˜ˆì¸¡ (Binary Classification)

## ğŸ¯ ìµœì¢… ì„±ì 

| ì œì¶œ | ìŠ¤ì½”ì–´ | ìˆœìœ„ | ì„¤ëª… |
|------|--------|------|------|
| **Supreme Evolved Refined** | **0.3434805649** | ìƒìœ„ê¶Œ | ìµœê³  ì„±ì  |
| Final Push v1 | 0.3434775373 | - | 2ìœ„ ìŠ¤ì½”ì–´ |
| Enhanced V2 | 0.3425593061 | - | ê°œì„ ëœ ì „ì²˜ë¦¬ ë²„ì „ |

**í‰ê°€ ì§€í‘œ**: AUC (Area Under the Curve)

## ğŸ›  ê¸°ìˆ  ìŠ¤íƒ

### í•µì‹¬ ê¸°ìˆ 
- **ì–¸ì–´**: Python 3.12
- **ML í”„ë ˆì„ì›Œí¬**: LightGBM, XGBoost
- **ë°ì´í„° ì²˜ë¦¬**: Pandas, NumPy
- **íŠ¹ì§• ì¶”ì¶œ**: Scipy, Scikit-learn

### ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬
```python
lightgbm==4.x      # GPU ê°€ì† Gradient Boosting
xgboost==2.x       # ì•™ìƒë¸” ë‹¤ì–‘ì„±
pandas==2.x        # ë°ì´í„° ì²˜ë¦¬
numpy==1.x         # ìˆ˜ì¹˜ ì—°ì‚°
scipy==1.x         # í†µê³„ ë° ë³€í™˜
```

## ğŸ“Š ë°ì´í„°ì…‹

- **Train**: 10.7M rows, ~8.8GB (Parquet)
- **Test**: 1.5M rows, ~1.3GB (Parquet)
- **Features**: ìµëª…í™”ëœ ì‚¬ìš©ì/ê´‘ê³ /ì»¨í…ìŠ¤íŠ¸ í”¼ì²˜
- **Target**: clicked (0 or 1)
- **íŠ¹ì§•**:
  - ì‹¤ì œ í† ìŠ¤ ì•± ê´‘ê³  ë…¸ì¶œ/í´ë¦­ ë¡œê·¸
  - ë³´ì•ˆìƒ í”¼ì²˜ëª… ìµëª…í™”
  - ë†’ì€ í´ë˜ìŠ¤ ë¶ˆê· í˜• (Click rate: 1.91%)

## ğŸ— ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 1. ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
```
Raw Data â†’ Missing Value Handling â†’ Feature Engineering â†’ Sampling â†’ Model Training
```

### 2. ëª¨ë¸ ì•™ìƒë¸” êµ¬ì¡°
```
â”œâ”€â”€ LightGBM (Primary)
â”‚   â”œâ”€â”€ 5-Fold Cross Validation
â”‚   â”œâ”€â”€ 5 Different Seeds
â”‚   â””â”€â”€ GPU Acceleration
â”œâ”€â”€ XGBoost (Diversity)
â”‚   â””â”€â”€ CUDA Support
â””â”€â”€ Weighted Ensemble
```

## ğŸš€ í•µì‹¬ ì „ëµ

### 1. Advanced Preprocessing
```python
# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
- ì¹´í…Œê³ ë¦¬í˜•: -999 (íŠ¹ë³„ê°’)
- ì—°ì†í˜•: Median Imputation

# ë°ì´í„° ìƒ˜í”Œë§
- Balanced Sampling (1:1)
- Imbalanced Sampling (1:1.5)
- Large Sample (3.5M)
```

### 2. Feature Engineering (42+ Features)

#### ìƒí˜¸ì‘ìš© í”¼ì²˜
- ê³±ì…ˆ, ë‚˜ëˆ—ì…ˆ, ì¡°í™”í‰ê· , ê¸°í•˜í‰ê· 
- ì˜ˆ: `history_a_1 Ã— history_b_21`

#### í†µê³„ í”¼ì²˜
- Mean, Std, Skewness, Kurtosis
- Quantiles (Q25, Q75, IQR)
- CV (Coefficient of Variation)
- MAD (Median Absolute Deviation)

#### ì‹œê°„ ì¸ì½”ë”©
```python
# ë‹¤ì¤‘ ì£¼ê¸° sin/cos ë³€í™˜
for period in [24, 12, 8, 6]:
    hour_sin = sin(2Ï€ Ã— hour / period)
    hour_cos = cos(2Ï€ Ã— hour / period)

# í”¼í¬ íƒ€ì„ indicator
- is_morning_rush (7-9ì‹œ)
- is_lunch (11-13ì‹œ)
- is_prime_time (19-22ì‹œ)
```

#### ë‹¤í•­ì‹ í”¼ì²˜
- ì œê³±, ì„¸ì œê³±, ì œê³±ê·¼
- log1p, exp ë³€í™˜

### 3. Model Training

**LightGBM ìµœì  íŒŒë¼ë¯¸í„°**
```python
{
    'objective': 'binary',
    'metric': 'auc',
    'num_leaves': 200,
    'learning_rate': 0.012,
    'feature_fraction': 0.65,
    'bagging_fraction': 0.75,
    'lambda_l1': 0.05,
    'lambda_l2': 0.05,
    'device': 'gpu',
    'num_boost_round': 2500
}
```

**ì•™ìƒë¸” ì „ëµ**
- 5-Fold Ã— 5 Seeds = 25ê°œ LightGBM ëª¨ë¸
- XGBoost ì¶”ê°€ (ë‹¤ì–‘ì„± í™•ë³´)
- ê°€ì¤‘ í‰ê·  (Balanced sample ê°€ì¤‘ì¹˜ ë†’ì„)

### 4. Calibration Strategy

6ê°€ì§€ Calibration ë°©ë²• ì ìš©:

1. **Supreme Optimal**: `0.248 + 0.504 Ã— rank`
2. **Refined**: `0.2478 + 0.5042 Ã— rank`
3. **Aggressive**: `0.252 + 0.496 Ã— rank`
4. **Quantile Transform**: Uniform distribution
5. **Power Law**: `rank^0.95`
6. **Sigmoid Stretch**: `sigmoid(8 Ã— (rank - 0.5))`

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
toss-ctr-prediction/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   â””â”€â”€ data_loader.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ lightgbm_trainer.py
â”‚   â”‚   â”œâ”€â”€ xgboost_trainer.py
â”‚   â”‚   â””â”€â”€ ensemble.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ calibration.py
â”‚   â”‚   â””â”€â”€ validation.py
â”‚   â””â”€â”€ main_training_pipeline.py
â”œâ”€â”€ submissions/
â”‚   â”œâ”€â”€ supreme_evolved_refined.csv
â”‚   â”œâ”€â”€ final_push_v1.csv
â”‚   â””â”€â”€ enhanced_v2.csv
â””â”€â”€ docs/
    â”œâ”€â”€ APPROACH.md          # ìƒì„¸ ì ‘ê·¼ ë°©ë²•
    â”œâ”€â”€ FEATURE_ENGINEERING.md
    â””â”€â”€ RESULTS_ANALYSIS.md
```

## ğŸ”¬ ì‹¤í—˜ ë° ê²°ê³¼

### ì£¼ìš” ì‹¤í—˜

| ì‹¤í—˜ | AUC Score | ê°œì„ ì  | ë¹„ê³  |
|------|-----------|--------|------|
| Baseline (22 features) | 0.3409 | - | ê¸°ë³¸ ëª¨ë¸ |
| + Feature Engineering | 0.3425 | +0.0016 | 28ê°œ í”¼ì²˜ |
| + Advanced FE (42+) | **0.3434** | **+0.0025** | **ìµœì¢… ë² ìŠ¤íŠ¸** |
| + Ensemble (25 models) | 0.3434 | +0.0000 | ì•ˆì •ì„± í–¥ìƒ |

### Feature Importance (Top 10)

```
1. history_b_21      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 12.3%
2. history_a_1       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  11.8%
3. feat_b_3          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   10.2%
4. feat_c_8          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     8.7%
5. history_b_30      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      7.5%
6. inventory_id      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       6.8%
7. feat_a_1          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        5.9%
8. age_group         â–ˆâ–ˆâ–ˆâ–ˆ         4.6%
9. hour              â–ˆâ–ˆâ–ˆâ–ˆ         4.3%
10. feat_b_1         â–ˆâ–ˆâ–ˆ          3.8%
```

## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### ì„±ê³µ ìš”ì¸
1. **ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§**: ë‹¨ìˆœ í”¼ì²˜ë³´ë‹¤ ìƒí˜¸ì‘ìš©/í†µê³„ í”¼ì²˜ê°€ íš¨ê³¼ì 
2. **ë‹¤ì–‘í•œ ìƒ˜í”Œë§ ì „ëµ**: Balanced, Imbalanced, Large ì¡°í•©
3. **ì•™ìƒë¸”ì˜ í˜**: ë‹¨ì¼ ëª¨ë¸ ëŒ€ë¹„ ì•ˆì •ì„± í¬ê²Œ í–¥ìƒ
4. **Calibration ìµœì í™”**: Rank-based calibrationì´ ê°€ì¥ íš¨ê³¼ì 

### ì‹¤íŒ¨ ë° êµí›ˆ
1. **ê³¼ë„í•œ í”¼ì²˜ ì¶”ê°€**: 93ê°œ í”¼ì²˜ ì‹œ ì˜¤íˆë ¤ ì„±ëŠ¥ í•˜ë½ (-0.11)
2. **ë‹¨ìˆœ ì•™ìƒë¸”ì˜ í•œê³„**: ë‹¨ìˆœ í‰ê· ë³´ë‹¤ ê°€ì¤‘ í‰ê· ì´ íš¨ê³¼ì 
3. **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ì²­í‚¹ ë° GC í•„ìˆ˜

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### í™˜ê²½ ì„¤ì •
```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

### í•™ìŠµ ì‹¤í–‰
```bash
# ê¸°ë³¸ í•™ìŠµ
python src/main_training_pipeline.py

# GPU ê°€ì† (CUDA í•„ìš”)
python src/main_training_pipeline.py --device gpu

# ì•™ìƒë¸” í•™ìŠµ
python src/models/ensemble.py --n_models 25
```

### ì˜ˆì¸¡ ìƒì„±
```bash
python src/predict.py --model_path models/best_model.pkl
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### GPU í™œìš©
- LightGBM GPU ëª¨ë“œ: í•™ìŠµ ì†ë„ 5-10ë°° í–¥ìƒ
- XGBoost CUDA: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ íš¨ìœ¨í™”

### ë©”ëª¨ë¦¬ ìµœì í™”
```python
# Float32 ì‚¬ìš©
X = X.astype(np.float32)

# ì²­í‚¹ ì²˜ë¦¬
for chunk in pd.read_parquet('train.parquet', chunksize=100000):
    process_chunk(chunk)

# Garbage Collection
del train_df
gc.collect()
```

## ğŸ“š ì°¸ê³  ìë£Œ

- [LightGBM Documentation](https://lightgbm.readthedocs.io/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [CTR Prediction Best Practices](https://arxiv.org/)

## ğŸ¤ ê¸°ì—¬

ì´ í”„ë¡œì íŠ¸ëŠ” Dacon x í† ìŠ¤ CTR ì˜ˆì¸¡ ëŒ€íšŒ ì°¸ê°€ì‘ì…ë‹ˆë‹¤.

## ğŸ“„ ë¼ì´ì„¼ìŠ¤

MIT License

---

**ê°œë°œ ê¸°ê°„**: 2025.09 ~ 2025.10
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025.10.13
