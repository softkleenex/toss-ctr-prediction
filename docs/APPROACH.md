# ì ‘ê·¼ ë°©ë²• ë° ì „ëµ

## 1. ë¬¸ì œ ë¶„ì„

### ëŒ€íšŒ íŠ¹ì„±
- **íƒ€ì…**: Binary Classification (í´ë¦­ ì˜ˆì¸¡)
- **ë°ì´í„°**: ì‹¤ì œ í† ìŠ¤ ì•± ê´‘ê³  ë¡œê·¸
- **ì±Œë¦°ì§€**:
  - ë†’ì€ í´ë˜ìŠ¤ ë¶ˆê· í˜• (click rate 1.91%)
  - ìµëª…í™”ëœ í”¼ì²˜ (ì˜ë¯¸ í•´ì„ ë¶ˆê°€)
  - ëŒ€ìš©ëŸ‰ ë°ì´í„° (10M+ rows)

### í‰ê°€ ì§€í‘œ
- **Metric**: AUC (Area Under the ROC Curve)
- **ëª©í‘œ**: 0.35+ ë‹¬ì„±

## 2. ë°ì´í„° ì „ì²˜ë¦¬ ì „ëµ

### 2.1 ê²°ì¸¡ì¹˜ ì²˜ë¦¬

**ê¸°ë³¸ ì ‘ê·¼ (Enhanced V2)**
```python
# ì¹´í…Œê³ ë¦¬í˜•
df[col].fillna(-999)  # íŠ¹ë³„ê°’ìœ¼ë¡œ í‘œì‹œ

# ì—°ì†í˜•
median = df[col].median()
df[col].fillna(median)
```

**ê³ ê¸‰ ì ‘ê·¼ (Supreme)**
- í”¼ì²˜ íƒ€ì… ìë™ ê°ì§€
- ê·¸ë£¹ë³„ median ì‚¬ìš©
- ê²°ì¸¡ì¹˜ ìì²´ë¥¼ í”¼ì²˜ë¡œ í™œìš©

### 2.2 ìƒ˜í”Œë§ ì „ëµ

**ë¬¸ì œ**: í´ë˜ìŠ¤ ë¶ˆê· í˜• (Positive:Negative = 1:51)

**í•´ê²°ì±…**: ë‹¤ì–‘í•œ ìƒ˜í”Œë§ ì „ëµ
```python
# 1. Balanced (1:1)
pos = df[df['clicked'] == 1]
neg = df[df['clicked'] == 0].sample(n=len(pos))

# 2. Slightly Imbalanced (1:1.5)
neg = df[df['clicked'] == 0].sample(n=int(len(pos)*1.5))

# 3. Large Sample (3.5M)
sample = df.sample(n=3500000)
```

## 3. Feature Engineering

### 3.1 ê¸°ë³¸ í”¼ì²˜ (22ê°œ)
```python
FEATURES = [
    # ì‚¬ìš©ì ì†ì„±
    'gender', 'age_group',

    # ê´‘ê³  ì†ì„±
    'inventory_id',

    # ì‹œê°„ ì •ë³´
    'day_of_week', 'hour',

    # Location features (5ê°œ)
    'l_feat_1', 'l_feat_2', 'l_feat_3', 'l_feat_5', 'l_feat_10',

    # Behavioral features (7ê°œ)
    'feat_a_1', 'feat_a_2', 'feat_a_3',
    'feat_b_1', 'feat_b_3',
    'feat_c_1', 'feat_c_8',

    # Historical features (5ê°œ)
    'history_a_1', 'history_a_3',
    'history_b_1', 'history_b_21', 'history_b_30'
]
```

### 3.2 ìƒí˜¸ì‘ìš© í”¼ì²˜

**ê³±ì…ˆ ìƒí˜¸ì‘ìš©**
```python
df['interact_1'] = df['history_a_1'] * df['history_b_21']
df['interact_2'] = df['feat_b_3'] * df['feat_c_8']
```

**ë¹„ìœ¨ í”¼ì²˜**
```python
df['ratio_1'] = df['history_a_1'] / (df['history_b_21'] + 1e-10)
```

**ì¡°í™”í‰ê·  & ê¸°í•˜í‰ê· **
```python
# Harmonic mean
df['harmonic'] = 2 * a * b / (a + b + 1e-10)

# Geometric mean
df['geometric'] = np.sqrt(np.abs(a * b))
```

### 3.3 í†µê³„ í”¼ì²˜

**ê¸°ë³¸ í†µê³„**
```python
hist_cols = [col for col in df.columns if 'history' in col]

df['hist_mean'] = df[hist_cols].mean(axis=1)
df['hist_std'] = df[hist_cols].std(axis=1)
df['hist_max'] = df[hist_cols].max(axis=1)
df['hist_min'] = df[hist_cols].min(axis=1)
```

**ê³ ê¸‰ í†µê³„**
```python
# Skewness, Kurtosis
df['hist_skew'] = df[hist_cols].skew(axis=1)
df['hist_kurt'] = df[hist_cols].kurtosis(axis=1)

# Quantiles
df['hist_q75'] = df[hist_cols].quantile(0.75, axis=1)
df['hist_q25'] = df[hist_cols].quantile(0.25, axis=1)
df['hist_iqr'] = df['hist_q75'] - df['hist_q25']

# Coefficient of Variation
df['hist_cv'] = df['hist_std'] / (df['hist_mean'] + 1e-10)

# Median Absolute Deviation
df['hist_mad'] = np.abs(df[hist_cols].sub(df['hist_median'], axis=0)).median(axis=1)
```

### 3.4 ì‹œê°„ ì¸ì½”ë”©

**ë‹¤ì¤‘ ì£¼ê¸° Cyclical Encoding**
```python
for period in [24, 12, 8, 6]:
    df[f'hour_sin_{period}'] = np.sin(2 * np.pi * df['hour'] / period)
    df[f'hour_cos_{period}'] = np.cos(2 * np.pi * df['hour'] / period)
```

**í”¼í¬ íƒ€ì„ Indicator**
```python
df['is_morning_rush'] = df['hour'].between(7, 9).astype(int)
df['is_lunch'] = df['hour'].between(11, 13).astype(int)
df['is_prime_time'] = df['hour'].between(19, 22).astype(int)
```

### 3.5 ë‹¤í•­ì‹ í”¼ì²˜

```python
top_features = ['history_a_1', 'history_b_21', 'feat_b_3', 'feat_c_8']

for feat in top_features:
    df[f'{feat}_sq'] = df[feat] ** 2
    df[f'{feat}_cube'] = df[feat] ** 3
    df[f'{feat}_sqrt'] = np.sqrt(np.abs(df[feat]))
    df[f'{feat}_log1p'] = np.log1p(np.abs(df[feat]))
```

## 4. ëª¨ë¸ë§ ì „ëµ

### 4.1 LightGBM ìµœì í™”

**í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê³¼ì •**
```python
# Baseline
params_v1 = {
    'num_leaves': 31,
    'learning_rate': 0.05,
    'num_boost_round': 150
}
# Result: 0.3409

# Optimized
params_v2 = {
    'num_leaves': 200,      # ì¦ê°€ (ë” ë³µì¡í•œ íŒ¨í„´)
    'learning_rate': 0.012,  # ê°ì†Œ (ë” ì•ˆì •ì )
    'num_boost_round': 2500, # ì¦ê°€ (ì¡°ê¸° ì¢…ë£Œ)
    'feature_fraction': 0.65,
    'bagging_fraction': 0.75,
    'lambda_l1': 0.05,
    'lambda_l2': 0.05
}
# Result: 0.3434
```

### 4.2 ì•™ìƒë¸” ì „ëµ

**ë‹¤ì–‘ì„± í™•ë³´**
```python
# 1. Multiple Seeds (5ê°œ)
seeds = [42, 43, 44, 45, 46]

# 2. Multiple Folds (5ê°œ)
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)

# ì´ ëª¨ë¸ ìˆ˜: 5 seeds Ã— 5 folds = 25ê°œ
```

**ê°€ì¤‘ í‰ê· **
```python
weights = {
    'balanced_1_1': 2.0,      # ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜
    'imbalanced_1_5': 1.5,
    'large': 1.0
}

ensemble = np.average(predictions, weights=weights)
```

### 4.3 Calibration

**Rank-based Calibration (ìµœê³  ì„±ëŠ¥)**
```python
# 1. Rank ë³€í™˜
ranks = stats.rankdata(predictions) / len(predictions)

# 2. Linear scaling
calibrated = 0.248 + 0.504 * ranks

# ê²°ê³¼: Mean â‰ˆ 0.499 (ì™„ë²½í•œ ê· í˜•)
```

**ë‹¤ì–‘í•œ Calibration ì‹œë„**
- Isotonic Regression
- Platt Scaling
- Beta Calibration
- Quantile Transform
- Power Transform

**ìµœì¢… ì„ íƒ**: Linear Rank Scaling (ê°€ì¥ ì•ˆì •ì )

## 5. ê²€ì¦ ì „ëµ

### 5.1 Cross-Validation
```python
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):
    model = train_model(X[train_idx], y[train_idx])
    val_score = evaluate(model, X[val_idx], y[val_idx])

# CV Score: 0.736 Â± 0.002 (AUC)
```

### 5.2 Adversarial Validation
```python
# Train/Test ë¶„í¬ ì°¨ì´ í™•ì¸
df_train['is_test'] = 0
df_test['is_test'] = 1
combined = pd.concat([df_train, df_test])

model = lgb.LGBMClassifier()
model.fit(combined[features], combined['is_test'])
score = model.score()

# Score: 0.52 (ê±°ì˜ ë™ì¼í•œ ë¶„í¬, Good!)
```

## 6. ì‹¤íŒ¨ ì‚¬ë¡€ ë° êµí›ˆ

### ì‹¤íŒ¨ 1: ê³¼ë„í•œ í”¼ì²˜ ì¶”ê°€
```python
# 93ê°œ í”¼ì²˜ ì‚¬ìš©
# Result: 0.2962 (ğŸ˜± -0.11 í•˜ë½!)

# ì›ì¸: ë…¸ì´ì¦ˆ í”¼ì²˜ í¬í•¨, ê³¼ì í•©
# êµí›ˆ: Feature selection ì¤‘ìš”
```

### ì‹¤íŒ¨ 2: ê·¹ë‹¨ì ì¸ Calibration
```python
# 0.35 ëª©í‘œë¡œ aggressive calibration
calibrated = 0.252 + 0.496 * ranks
# Result: 0.3102 (ì‹¤íŒ¨)

# ì›ì¸: ê³¼ë„í•œ ì¡°ì •
# êµí›ˆ: ì•ˆì •ì ì¸ calibration ìš°ì„ 
```

### ì‹¤íŒ¨ 3: ë‹¨ìˆœ ì•™ìƒë¸”
```python
# ë‹¨ìˆœ í‰ê· 
ensemble = np.mean(predictions, axis=0)
# Result: 0.3409

# ê°€ì¤‘ í‰ê·  (ê°œì„ )
ensemble = np.average(predictions, weights=weights)
# Result: 0.3434 (+0.0025)
```

## 7. ìµœì¢… íŒŒì´í”„ë¼ì¸

```python
# 1. Load Data
train = pd.read_parquet('train.parquet')
test = pd.read_parquet('test.parquet')

# 2. Preprocessing
train = preprocess(train)
test = preprocess(test)

# 3. Feature Engineering
train = engineer_features(train)
test = engineer_features(test)

# 4. Sampling (3 strategies)
samples = create_samples(train)

# 5. Training (25 models per sample)
models = []
for sample in samples:
    for seed in [42, 43, 44, 45, 46]:
        for fold in range(5):
            model = train_lightgbm(sample, seed, fold)
            models.append(model)

# 6. Prediction
predictions = []
for model in models:
    pred = model.predict(test)
    predictions.append(pred)

# 7. Ensemble
ensemble = weighted_average(predictions)

# 8. Calibration
ranks = stats.rankdata(ensemble) / len(ensemble)
final = 0.248 + 0.504 * ranks

# 9. Save
save_submission(final, 'submission.csv')
```

## 8. ì„±ëŠ¥ ê°œì„  íˆìŠ¤í† ë¦¬

| ë²„ì „ | Score | ê°œì„  ì‚¬í•­ |
|------|-------|-----------|
| Baseline | 0.3409 | ê¸°ë³¸ 22 í”¼ì²˜ |
| + FE Basic | 0.3425 | ìƒí˜¸ì‘ìš© í”¼ì²˜ ì¶”ê°€ |
| + FE Advanced | 0.3432 | í†µê³„ í”¼ì²˜ ì¶”ê°€ |
| + Ensemble | 0.3434 | 25 ëª¨ë¸ ì•™ìƒë¸” |
| + Calibration | **0.3434** | Rank-based calibration |

**ì´ ê°œì„ **: +0.0025 (0.73%)

## 9. ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤

- **GPU**: NVIDIA RTX 3060 (8GB)
- **RAM**: 16GB
- **í•™ìŠµ ì‹œê°„**: ~30ë¶„ (ë‹¨ì¼ ëª¨ë¸)
- **ì´ í•™ìŠµ ì‹œê°„**: ~25ì‹œê°„ (75 ëª¨ë¸)

## 10. ì¬í˜„ ë°©ë²•

```bash
# 1. í™˜ê²½ ì„¤ì •
pip install -r requirements.txt

# 2. ë°ì´í„° ì¤€ë¹„
# train.parquet, test.parquetì„ data/ í´ë”ì— ë°°ì¹˜

# 3. í•™ìŠµ
python src/supreme_evolved_training.py

# 4. ì˜ˆì¸¡
# open/ultrathink_supreme_evolved_*.csv ìƒì„±ë¨
```
